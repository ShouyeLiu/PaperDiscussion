#+title: Summary of Statistical Distribution
#+categories: ["Algorithm"]
#+tags: ["MathDistribution"]
#+draft: true
#+STARTUP: overview

#+LATEX_CLASS: article
#+LATEX_CLASS_OPTIONS: [a4paper]
#+LaTex_HEADER: \usepackage[ruled,vlined,boxed, commentsnumbered,norelsize,linesnumbered]{algorithm2e}
#+LaTex_HEADER: \usepackage{xcolor}
#+LaTex_HEADER: \usepackage[margin=0.5in]{geometry} % set the margin of document
#+LaTex_HEADER: \hypersetup{ colorlinks=true, linkcolor=violet }
# begin laber submatrix command:
#+LaTex_HEADER: \usepackage{CJKutf8}
#+LaTex_HEADER: \usepackage{amsmath,amsfonts,amssymb,amsthm}
#+LaTex_HEADER: \usepackage{xcolor}
#+LaTex_HEADER: \definecolor{ocre}{RGB}{0,173,239}
#+LaTex_HEADER: \usepackage{blkarray}
#+LaTex_HEADER: \usepackage{tikz}
#+LaTex_HEADER: \usetikzlibrary{arrows,matrix,positioning,fit}

#+LaTex_HEADER: \usepackage[customcolors]{hf-tikz}
# end laber submatrix command:
#+OPTIONS: num:t toc:t H:4
#+OPTIONS: ^:nil

[[http://www.math.wm.edu/~leemis/chart/UDR/UDR.html][Distribution Chart]]

* Useful function
** Power function
Real functions of the form $f(x)=x^{a}$
** Exponential function
$f(x) = a^{x}$, for example, $f(x) = e^x$.
** Logrithm function
$log_b^y = x$

** Gamma function
$\Gamma(\alpha) = \int^{\infty}_0 t^{\alpha - 1} e^{-t} dt$

* Univariate Distribution relationship
[[http://www.math.wm.edu/~leemis/chart/UDR/UDR.html][Chart for univariate distribution relationship]]
* Relationship between discrete and continuous distribution
#+begin_src latex
\begin{CJK*}{UTF8}{gbsn}
连续分布几乎都能找到对应的离散分布：\\
贝努利实验：投硬币，正面反面概率分别为p，1-p；\\
二项分布：如果把贝努利实验连续做n次，出现正面的次数服从的分布；\\
二项分布的极限：泊松分布。给定时间内时间发生次数的分布；\\
负二项分布：在贝努利实验中，如果想让正面出现n次，需要做的实验次数的分布；\\
负二项分布的极限：gamma分布。问如果指定事件出现N次，需要等待的时间；\\
几何分布：n重贝努利实验，正面第一次出现时的实验次数；\\
几何分布的极限：指数分布。事件第一次发生等待的时间；\\
二项分布与负二项分布的关系：二项分布是在固定实验次数情况下，问结果分布。负二项分布是在固定结果情况下问实验次数分布。一个是在固定投入问产出，一个是在固定产出下问投入。\\
几何分布与负二项分布的关系：负二项分布是N个几何分布的和。相当于做N次几何分布，事件正好发生N次。各几何分布的实验次数之和就是负二项分布的总次数。\\
指数分布与gamma分布的关系：N次指数分布时间之和就是gamma分布中事件发生N次等待总时间。
\end{CJK*}
#+end_src
** Table 1. Relationship abbreviation
|-------------------+-------------+-----------|
| Discrete          | Continuous  | shorthand |
|-------------------+-------------+-----------|
| Binomial          | Poisson     | BP        |
| Negative Binomial | Gamma       | NG        |
| Geometric         | Exponential | GE        |
|-------------------+-------------+-----------|

* Continuous distribution
** Exponential distribution
The probability density function (pdf) of an exponential distribution is
\begin{align}
f(x;\lambda) = \left\{ \begin{array}{rcl}
         \lambda e^{ -\lambda x } & x \geq 0 \\
           0  & x < 0
                \end{array}\right
\end{align}
Or,
\begin{align}
f(x;\lambda) = \left\{ \begin{array}{rcl}
         \frac{1}{\beta} e^{ - x/\beta} & x \geq 0 \\
           0  & x < 0
                \end{array}\right
\end{align}
where $\lambda$ is rate parameter, $\beta$ is scale parameter.

** Gamma distribution
Let $t = \beta x$,
#+begin_src latex
\begin{align}
\Gamma(\alpha, \beta) &= \int^{\infty}_0 (\beta x)^{\alpha - 1} e^{-\beta x} d(\beta x) \\
  &= \int^{\infty}_0 x^{\alpha - 1} e^{-\beta x}\beta^{\alpha - 1} \beta  dx  \\
   &= \beta^{\alpha} \int^{\infty}_0 x^{\alpha - 1} e^{-\beta x}  dx
\end{align}
#+end_src
the probability density function is
\begin{align}
f(x;\alpha,\beta) = \left\{ \begin{array}{rcl}
         \frac{\beta^{\alpha}}{\Gamma{(\alpha)}} x^{\alpha -1} e^{-\beta x} & x \geq 0 \\
           0  & x < 0
    \end{array}\right
\end{align}
where $\alpha$ is shape parameter and $\beta$ is rate parameter.

** Normal distribution
*** The derivative of normal distribution (Ref: [[https://math.stackexchange.com/questions/384893/how-was-the-normal-distribution-derived/385427#385427?newreg=3bf27de39aeb420ba221d562d38c5ccd][Tim]] )
Suppose I throw a dart into a dartboard. I aim at the centre of the board (0,0) but I'm not all that good with darts so the dart lands in a random position (X,Y) which has a joint density function $f:\mathbb R^2\to\mathbb R^+$.
Let's make two assumptions about the way I play darts.

- The density is rotationally invariant so the distribution of where my dart lands only depends on the distance of the dart to the centre.
- The random variables $X$ and $Y$ are independent, how much I miss left and right makes no difference to the distribution of how much I miss up and down.

  So by assumption one and Pythagoras I must be able to express the density
  \begin{align}
   f(x,y) = g(x^2 + y^2).
  \end{align}
  Now as the random variable $X$ and $Y$ are independent and identically distributed I must be able to express
  \begin{align}
  f(x,y) \propto f(x,0) f(0,y)
  \end{align}
  Combining these assumptions we get that for every pair (x,y), we have
  \begin{align}
   g(x^2 + y^2) \propto g(x^2) g(y^2).
  \end{align}
  This means that $g$ must be an exponential function
  \begin{align}
   g(t) = Ae^{-Bt}
  \end{align}
  So $A$ will be some normalising constant. $B$ somehow reflects the units I'm measuring in. ( So if I measure the distance in cm $B$ will be 10 times as big as if I measured in mm). $B$ must be negative because the density should be a decreasing function of distance(I'm not that bad at darts).

  So to work out $A$, I need to integrate $f(.,.)$ over $\mathbb{R}^2$ a quick change of coordinates and
  \begin{align}
  \iint_{\mathbb{R}} f(x,y)dxdy = 2\pi \int^{\infty}_{0} tg(t)dt = \frac{2\pi}{B^2}
  \end{align}
  So we should set $A = \frac{B^2}{2\pi}$ it's convenient to choose $B$ in terms of the standart deviation, so we set $B = \frac{1}{2\sigma}$ and $A = \frac{1}{2\pi\sigma^2}$.
  So if I set $\tilde{f}(x) = \frac{1}{\sqrt(2\pi)\sigma}e^{-\frac{x^2}{2\sigma}}$ then $f(x,y) = \tilde{f}(x) \tilde{f}(y)$.

  So the $e$ comes from the fact I wanted my $X$ and $Y$ coordinates to be independent and the $\pi$ comes from the fact that I wanted rotational invariance so I'm integrating over a circle.

The interesting thing happens if I throw two darts. Suppose I throw my first dart aiming at (0,0) which lands at ($X_1$,$Y_1$), I aim my next dart at the first dart, so this one lands at ($X_2$,$Y_2$) with $X_2=X_1 + X$ and $X_2=Y_1+Y$.

So the position of the second dart is the sum of the two errors. But my sum is still rotationally invariant and the variables $X_2$ and $Y_2$ are still independent, so ($X_2$,$Y_2$) satisfies my two assumptions.

That means that when I add independent normal distributions together I get another normal distribution.

It's this property that makes it so useful, because if I take the average of a very long sequence of random variables I should get something that's the same shape no matter how long my sequence is and taking a sequence twice as long is like adding the two sequences together. It's this property of the normal distribution that makes it so useful.

*** Other useful materials
[[https://www.maa.org/sites/default/files/pdf/upload_library/22/Allendoerfer/stahl96.pdf][The evolution of the normal distribution]]

** $\chi^2$ distribution
The $\chi^2$ distribution is a special case of the gamma distribution, as a $\chi^2$ random variable with $n$ degrees of freedom follows a gamma distribution with shape parameter $\alpha = n/2$ and $\beta = 1/2$, namely, $\chi^2 \sim Gamma(n/2,1/2)$, giving the density function as
\begin{align}
f(x;\alpha,\beta) = \left\{ \begin{array}{rcl}
         \frac{2^{-n/2}}{\Gamma{(n/2)}} x^{n/2 -1} e^{-1/2 x} & x \geq 0 \\
           0  & x < 0
    \end{array}\right
\end{align}

** The relationship between normal and $\chi^2$ distribution
Firstly, we need prove $\Gamma(1/2) = \sqrt{\pi}$ (Ref: [[https://onlinehw.math.ksu.edu/math340book/chap3/gamma.php#:~:text=So%20the%20Gamma%20function%20is,%2F2][The Gamma Function]])
\begin{align}
\Gamma(1/2)^2 &= \left(\int_0^{\infty}t^{-1/2}e^{-t}\,dt\right)^2 \\
&= \left(2\int_0^{\infty}e^{-x^2}\,dx\right)^2 && \text{let $t = x^2$} \\
&= \left(\int_{-\infty}^{\infty}e^{-x^2}\,dx\right)^2 && \text{from (0,$\infty$) to (-$\infty$,+$\infty$)} \\
&= \left(\int_{-\infty}^{\infty}e^{-x^2}\,dx\right)\left(\int_{-\infty}^{\infty}e^{-y^2}\,dy\right) \\
&= \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}e^{-x^2}e^{-y^2}\,dxdy \\
&= \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}e^{-(x^2+y^2)}\,dxdy \\
&= \int_{0}^{2\pi}\int_{0}^{\infty}e^{-r^2}\,rdrd\theta  && \text{let $x= r \cdot \cos(\theta)$ and $y= r \cdot \sin(\theta)
$}   \\
&= \frac12\int_{0}^{2\pi}\int_{0}^{\infty}e^{-u}\,dud\theta \tag{f} \\
&= \frac12\int_{0}^{2\pi}\left.-e^{-u}\right|_0^{\infty}\,d\theta \\
&= \frac12\int_{0}^{2\pi}\,d\theta \\
&= \pi
\end{align}
Secondly, we need to find the pdf of $X^2$ (Ref: [[https://math.stackexchange.com/questions/71537/derivation-of-chi-squared-pdf-with-one-degree-of-freedom-from-normal-distributio][Normal to Chi]])

If $X \sim \mathcal{N}(0,1)$, then the pdf of $X$ is
\begin{align}
\varphi(x) = \frac{1}{\sqrt{2\pi}} e^{-x^2/2}
\end{align}
Let $f$ be the pdf of $X^2$. Then,
\begin{align}
f(x) & = \frac{d}{dx} \Pr(X^2 \le x)= \frac{d}{dx} \Pr(-\sqrt{x}\le X\le\sqrt{x}) \\
& = \frac{d}{dx} \frac{1}{\sqrt{2\pi}} \int_{-\sqrt{x}}^{\sqrt{x}} e^{-u^2/2} \;du  \\
& = \frac{2}{\sqrt{2\pi}}\frac{d}{dx} \int_0^{\sqrt{x}}  e^{-u^2/2} \;du \\
& = \frac{2}{\sqrt{2\pi}} e^{-\sqrt{x}^2/2} \frac{d}{dx} \sqrt{x} = \frac{2}{\sqrt{2\pi}} e^{-x/2} \frac{1}{2\sqrt{x}}  = \frac{e^{-x/2}}{\sqrt{2\pi x}} \\
& = \frac{2^{-1/2}}{\Gamma{(1/2)}} x^{\frac12 - 1}e^{-x/2} \sim Gamma(1/2,1/2)
\end{align}

** Noncentral $\chi^2$ distribution

** Wishart distribution

* Discrete distribution
** Bernoulli distribution $X \sim Bernoulli(p)$
Bernoulli distribution is used to indicate that the random variable $X$ has the Bernoulli distribution with parameter $p$, where $0 < p < 1$. A Bernoulli random variable $X$ with success probability $p$ has probability mass function
#+begin_src latex
\begin{align}
f(x) = p^x(1 - p)^{1 -x} \quad\quad\quad\quad x = 0,1
\end{align}
#+end_src
where $0 < p < 1$. The Bernoulli distribution is associated with the notion of a $Bernoulli trial$, which is an experiment with two outcomes, generically referred to as $success$(x = 1) and $failure$(x = 0).
** Binomial distribution $X \sim Binomial(n,p)$
The binomial distribution models the number of successes in $n$ mutually independent Bernoulli trials, each with probability of success $p$. The random variable $X \sim binomial(n,p)$ has probability mass function
#+begin_src latex
\begin{align}
f(x) =  \begin{pmatrix} n \\ x \end{pmatrix} p^x(1 - p)^{n -x} \quad\quad\quad\quad x = 0,1, \dots, n
\end{align}
#+end_src

** Possion distribution
A Poisson random variable $X$ with scale parameter $\mu$ has probability mass function
#+begin_src latex
\begin{align}
f(x) = \frac{\mu^xe^{-u}}{x!} \quad\quad\quad\quad x = 0,1,2,\cdots
\end{align}
#+end_src
The Poisson distribution can be used to approximate the binomial distribution when $n$ is large and $p$ is small. Besides, it can be also used to model the number of events in an interval associated with a process that evolves randomly over space or time. Applications include the number of typographical errors in a book, the number of customers arriving in an hour.

* P value distribution
[[https://en.wikipedia.org/wiki/Order_statistic#:~:text=Order%20statistics%20sampled%20from%20an%20uniform%20distribution,-In%20this%20section&text=that%20is%2C%20the%20kth%20order,a%20beta%2Ddistributed%20random%20variable.&text=and%20the%20result%20follows.,%2F%20(n%20%2B%201)][ordered statistics]], [[https://genome.sph.umich.edu/wiki/Code_Sample:_Generating_QQ_Plots_in_R][QQ plot]]
